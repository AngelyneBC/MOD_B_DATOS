{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV a txts\n",
    "\n",
    "Vamos a generar los inserts para las tablas de Autosummit Perú SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrir CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ASP = pd.read_csv('ASP.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                (42206340) DANIELLA MARIA BOLAÑOS GAMERO\n",
       "1                         (20100115663) PANDERO S.A. EAFC\n",
       "2       (09468059) KATIA NATHALI DE LOAYZA WONG DE PAC...\n",
       "3        (46472213) JHONATHAN MITCHELL ANTEZANA ESCALANTE\n",
       "4                (42607724) KRISCIA ZULAY REATEGUI ZAMORA\n",
       "                              ...                        \n",
       "1253                                                  NaN\n",
       "1254                                                  NaN\n",
       "1255                                                  NaN\n",
       "1256                                                  NaN\n",
       "1257                                                  NaN\n",
       "Name: Cliente, Length: 1258, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_ASP['Cliente']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Filtrado de colores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_rep = ds_ASP['Color']\n",
    "colors = []\n",
    "\n",
    "for c in colors_rep:\n",
    "    if isinstance(c, str) and c.strip() != '' and c.lower() != 'nan':\n",
    "        if c not in colors:\n",
    "            colors.append(c)\n",
    "\n",
    "colors = sorted(colors)\n",
    "\n",
    "text_colors = '-- migrate:up\\n\\n'\n",
    "id = 1\n",
    "\n",
    "for c in colors:\n",
    "    c_escaped = c.replace(\"'\", \"''\")\n",
    "    text_colors += f\"INSERT INTO colors (id, nombre) VALUES ({id}, '{c_escaped}');\\n\"\n",
    "    id += 1\n",
    "\n",
    "text_colors += '\\n-- migrate:down\\nDELETE FROM colors;'\n",
    "with open('inserts_colors.sql', 'w') as f:\n",
    "    f.write(text_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Filtrado por Asesor </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "asesores_rep = ds_ASP['Asesor']\n",
    "asesores = []\n",
    "\n",
    "#En caso tenga nombres \"raros\" el asesor\n",
    "PALABRAS_INVALIDAS = {\n",
    "    \"PDI\", \"EXHIBICION\", \"ATE\", \"CASO\", \"ENTREGA\"\n",
    "}\n",
    "\n",
    "#Almacenar todo en diccionario - para evitar duplicados\n",
    "asesores = {} \n",
    "\n",
    "def es_asesor_valido(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return False\n",
    "\n",
    "    texto = texto.strip().upper()\n",
    "\n",
    "    # Debe tener al menos un espacio (nombre + apellido)\n",
    "    if \" \" not in texto:\n",
    "        return False\n",
    "\n",
    "    # No debe contener comas ni números\n",
    "    if \",\" in texto or re.search(r\"\\d\", texto):\n",
    "        return False\n",
    "\n",
    "    # No debe contener palabras inválidas\n",
    "    for palabra in PALABRAS_INVALIDAS:\n",
    "        if palabra in texto:\n",
    "            return False\n",
    "\n",
    "    # Solo letras y espacios\n",
    "    if not re.match(r\"^[A-ZÁÉÍÓÚÑ ]+$\", texto):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "for c in asesores_rep:\n",
    "    if not es_asesor_valido(c):\n",
    "        continue\n",
    "\n",
    "    c = c.strip().upper()\n",
    "    nombres, apellidos = c.split(\" \", 1)\n",
    "\n",
    "    key = (nombres, apellidos)\n",
    "    asesores[key] = True\n",
    "\n",
    "sql_asesores = \"-- migrate:up\\n\\n\"\n",
    "id_asesor = 1\n",
    "\n",
    "for nombres, apellidos in sorted(asesores.keys()):\n",
    "    sql_asesores += (\n",
    "        \"INSERT INTO asesores (id, nombres, apellidos) \"\n",
    "        f\"VALUES ({id_asesor}, '{nombres}', '{apellidos}');\\n\"\n",
    "    )\n",
    "    id_asesor += 1\n",
    "\n",
    "sql_asesores += \"\\n-- migrate:down\\nDELETE FROM asesores;\\n\"\n",
    "\n",
    "with open(\"inserts_asesores.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_asesores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Descomposición en marca (nombre) y modelo (nombre, version, año)</h3>\n",
    "\n",
    "<p> Considerar que Marca no está de forma explícita en el CSV. Solución: inferir y filtrar con diccionarios </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# =========================\n",
    "# 1. COLUMNA CSV\n",
    "# =========================\n",
    "modelos_rep = ds_ASP['Modelo']\n",
    "\n",
    "# Diccionarios (evitan duplicados automáticamente)\n",
    "marcas = {}      # { \"FORD\": 1 }\n",
    "modelos = {}     # { (modelo_base, version, traccion, anio, motor, marca): True }\n",
    "\n",
    "# =========================\n",
    "# CATÁLOGOS CONTROLADOS\n",
    "# =========================\n",
    "MAPA_MARCAS = {\n",
    "    \"FORD\": [\n",
    "        \"TERRITORY\", \"RANGER\", \"F-150\", \"MAVERICK\",\n",
    "        \"EXPLORER\", \"ESCAPE\", \"BRONCO\",\n",
    "        \"MUSTANG\", \"EXPEDITION\"\n",
    "    ],\n",
    "    \"CHERY\": [\n",
    "        \"TIGGO\", \"ARRIZO\", \"M7\", \"HIMLA\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "VERSIONES_VALIDAS = {\n",
    "    \"TITANIUM\", \"TREND\", \"XLS\", \"XLT\", \"XL\", \"LTD\",\n",
    "    \"PLATINUM\", \"RAPTOR\", \"BADLANDS\", \"LARIAT\",\n",
    "    \"ACTIVE\", \"ST\", \"PREMIUM\", \"PRO\", \"MAX\",\n",
    "    \"BIG\", \"BEND\", \"TREMOR\", \"GT\"\n",
    "}\n",
    "\n",
    "TRACCIONES = {\"4X2\", \"4X4\", \"AWD\", \"4WD\"}\n",
    "DESCARTES = {\"MT\", \"AT\", \"CVT\", \"DCT\", \"FHEV\", \"MHEV\", \"PHEV\", \"GLP\", \"GNV\"}\n",
    "\n",
    "# =========================\n",
    "# FUNCIONES\n",
    "# =========================\n",
    "\n",
    "def detectar_marca(texto: str):\n",
    "    texto = texto.upper()\n",
    "    for marca, modelos in MAPA_MARCAS.items():\n",
    "        for m in modelos:\n",
    "            if m in texto:\n",
    "                return marca\n",
    "    return None\n",
    "\n",
    "\n",
    "def extraer_anio(texto: str):\n",
    "    match = re.search(r\"(20\\d{2})\", texto)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def separar_modelo(texto: str):\n",
    "    tokens = texto.upper().split()\n",
    "\n",
    "    motor = None\n",
    "    traccion = None\n",
    "    usados = set()\n",
    "\n",
    "    for t in tokens:\n",
    "        # Motor (1.5L, 2.0T, etc.)\n",
    "        if re.match(r\"\\d\\.\\d(T|L)?\", t):\n",
    "            motor = float(re.findall(r\"\\d\\.\\d\", t)[0])\n",
    "            usados.add(t)\n",
    "\n",
    "        # Tracción\n",
    "        elif t in TRACCIONES:\n",
    "            traccion = t\n",
    "            usados.add(t)\n",
    "\n",
    "        # Tokens técnicos descartables\n",
    "        elif t in DESCARTES:\n",
    "            usados.add(t)\n",
    "\n",
    "    # limpiar tokens ya usados\n",
    "    limpio = [t for t in tokens if t not in usados]\n",
    "\n",
    "    modelo_base = limpio[0]\n",
    "    version_tokens = [t for t in limpio[1:] if t in VERSIONES_VALIDAS]\n",
    "    version_modelo = \" \".join(version_tokens)\n",
    "\n",
    "    return modelo_base, version_modelo, traccion, motor\n",
    "\n",
    "# =========================\n",
    "# PROCESAMIENTO DEL CSV\n",
    "# =========================\n",
    "\n",
    "for fila in modelos_rep:\n",
    "    if not isinstance(fila, str) or fila.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    fila = fila.strip().upper()\n",
    "\n",
    "    marca = detectar_marca(fila)\n",
    "    anio = extraer_anio(fila)\n",
    "\n",
    "    if not marca or not anio:\n",
    "        continue\n",
    "\n",
    "    modelo_base, version, traccion, motor = separar_modelo(fila)\n",
    "\n",
    "    if not all([modelo_base, version, traccion, motor]):\n",
    "        continue\n",
    "\n",
    "    # registrar marca (PK lógica)\n",
    "    if marca not in marcas:\n",
    "        marcas[marca] = len(marcas) + 1\n",
    "\n",
    "    key = (modelo_base, version, traccion, anio, motor, marca)\n",
    "    modelos[key] = True\n",
    "\n",
    "# =========================\n",
    "# SQL: MARCAS\n",
    "# =========================\n",
    "\n",
    "sql_marcas = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for marca, id_marca in marcas.items():\n",
    "    sql_marcas += (\n",
    "        f\"INSERT INTO marcas (id_marca, nombre) \"\n",
    "        f\"VALUES ({id_marca}, '{marca}');\\n\"\n",
    "    )\n",
    "\n",
    "sql_marcas += \"\\n-- migrate:down\\nDELETE FROM marcas;\\n\"\n",
    "\n",
    "with open(\"inserts_marcas.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_marcas)\n",
    "\n",
    "# =========================\n",
    "# SQL: MODELOS\n",
    "# =========================\n",
    "\n",
    "sql_modelos = \"-- migrate:up\\n\\n\"\n",
    "id_modelo = 1\n",
    "\n",
    "for (modelo_base, version, traccion, anio, motor, marca) in modelos.keys():\n",
    "    marca_id = marcas[marca]\n",
    "\n",
    "    sql_modelos += (\n",
    "        \"INSERT INTO modelos \"\n",
    "        \"(id_modelo, modelo_base, version_modelo, traccion, anio, motor, marca_id) \"\n",
    "        f\"VALUES ({id_modelo}, '{modelo_base}', '{version}', \"\n",
    "        f\"'{traccion}', {anio}, {motor}, {marca_id});\\n\"\n",
    "    )\n",
    "    id_modelo += 1\n",
    "\n",
    "sql_modelos += \"\\n-- migrate:down\\nDELETE FROM modelos;\\n\"\n",
    "\n",
    "with open(\"inserts_modelos.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_modelos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtrado de GPS</h2>\n",
    "<p>Eliminar si tiene la palabra \"GPS\" para dejar el resto del texto </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def limpiar_gps(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return None\n",
    "\n",
    "    texto = texto.strip().upper()\n",
    "\n",
    "    # Eliminar fechas entre paréntesis\n",
    "    texto = re.sub(r\"\\(.*?\\)\", \"\", texto)\n",
    "\n",
    "    #Eliminar comentarios\n",
    "    texto = re.sub(r\"\\d+/\\d+\", \"\", texto)\n",
    "\n",
    "    #PALABRAS NO ACEPTADAS: Eliminar si existen\n",
    "    FiltroNoAceptado = [\n",
    "        \"OK\", \"INSTALADO\", \"COORDINAR\", \"CLIENTE\",\n",
    "        \"NO APLICA\", \"CAMPAÑA\", \"AUTOPLAN\"\n",
    "    ]\n",
    "\n",
    "    for palabra in FiltroNoAceptado:\n",
    "        texto = texto.replace(palabra, \"\")\n",
    "\n",
    "    #Solo queremos letras y espacios\n",
    "    texto = re.sub(r\"[^A-Z ]\", \"\", texto)\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "\n",
    "    return texto #La versión limpia del texto que pasa\n",
    "\n",
    "def detectar_gps_base(texto):\n",
    "    GPS_VALIDOS = {\"COMSATEL\",\"HUNTER\", \"SUPRA\",\"PANDERO\", \"PROTEMAX\", \"MAQUISISTEMAS\",\"EURORENTING\"}\n",
    "    \n",
    "    # Forma limpia: 'GPS {PALABRACLAVE}'\n",
    "    for palabraGPS in GPS_VALIDOS:\n",
    "        if palabraGPS in texto: \n",
    "            return f\"GPS {palabraGPS}\"\n",
    "    return None #Si no encaja, no es válido\n",
    "\n",
    "ds = pd.read_csv(\"ASP.csv\", encoding=\"utf-8\")\n",
    "\n",
    "gps_dict = {}\n",
    "id_gps = 1\n",
    "\n",
    "for g in ds['GPS']:\n",
    "    nombreGPSLimpio = limpiar_gps(g)\n",
    "    if not nombreGPSLimpio:\n",
    "        continue\n",
    "\n",
    "    gps_base = detectar_gps_base(nombreGPSLimpio)\n",
    "\n",
    "    if not gps_base:\n",
    "        continue \n",
    "\n",
    "    if gps_base not in gps_dict:\n",
    "        gps_dict[gps_base] = id_gps\n",
    "        id_gps += 1\n",
    "\n",
    "# Generar SQL GPS\n",
    "\n",
    "sql_gps = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for nombre, id_ in gps_dict.items():\n",
    "    sql_gps += (\n",
    "        \"INSERT INTO gps (id, nombre) \"\n",
    "        f\"VALUES ({id_}, '{nombre}');\\n\"\n",
    "    )\n",
    "\n",
    "sql_gps += \"\\n-- migrate:down\\nDELETE FROM gps;\\n\"\n",
    "\n",
    "with open(\"inserts_gps.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_gps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtrado de Clientes </h2>\n",
    "<p>Al estar en cliente el nombre de empresa / persona natural + DNI/RUC El objetivos principal será separar número de identificación de identidad y el nombre, así como clasificar en caso sea persona natural o empresa </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clientes_rep = ds_ASP['Cliente']\n",
    "\n",
    "# ===================== FUNCIONES =====================\n",
    "\n",
    "def limpiar_cliente(texto):\n",
    "    \"\"\"\n",
    "    (20100115663) PANDERO S.A. EAFC\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        return None, None\n",
    "\n",
    "    texto = texto.strip()\n",
    "\n",
    "    match = re.match(r\"\\((\\d+)\\)\\s*(.+)\", texto)\n",
    "    if not match:\n",
    "        return None, None\n",
    "\n",
    "    numero = match.group(1)\n",
    "    nombre = match.group(2).strip().upper()\n",
    "\n",
    "    return numero, nombre\n",
    "\n",
    "\n",
    "PALABRAS_EMPRESA = {\n",
    "    \"S.A\", \"S.A.\", \"S.A.C\", \"SAC\", \"SOCIEDAD\",\n",
    "    \"EMPRESA\", \"E.A.F.C\", \"EAFC\", \"CORPORACION\",\n",
    "    \"GRUPO\", \"GROUP\", \"E.I.R.L\", \"SERVICIOS\",\n",
    "    \"SRL\", \"S.R.L\"\n",
    "}\n",
    "\n",
    "def es_empresa(nombre):\n",
    "    return any(p in nombre for p in PALABRAS_EMPRESA)\n",
    "\n",
    "\n",
    "# ===================== ESTRUCTURAS =====================\n",
    "\n",
    "clientes = {}          # id_cliente -> (numero, nombre)\n",
    "personas = set()       # ids\n",
    "empresas = set()       # ids\n",
    "\n",
    "id_cliente = 1\n",
    "\n",
    "# ===================== PROCESAMIENTO =====================\n",
    "\n",
    "for fila in clientes_rep:\n",
    "    numero, nombre = limpiar_cliente(fila)\n",
    "\n",
    "    if not numero or not nombre:\n",
    "        continue\n",
    "\n",
    "    clientes[id_cliente] = (numero, nombre)\n",
    "\n",
    "    if es_empresa(nombre):\n",
    "        empresas.add(id_cliente)\n",
    "    else:\n",
    "        personas.add(id_cliente)\n",
    "\n",
    "    id_cliente += 1\n",
    "\n",
    "\n",
    "# ===================== SQL CLIENTES =====================\n",
    "\n",
    "sql_clientes = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for id_cliente, (numero, nombre) in clientes.items():\n",
    "    nombre = nombre.replace(\"'\", \"''\")\n",
    "    sql_clientes += (\n",
    "        \"INSERT INTO clientes (id_cliente, Numero_Identificacion, nombre) \"\n",
    "        f\"VALUES ({id_cliente}, '{numero}', '{nombre}');\\n\"\n",
    "    )\n",
    "\n",
    "sql_clientes += \"\\n-- migrate:down\\nDELETE FROM clientes;\\n\"\n",
    "\n",
    "with open(\"inserts_clientes.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_clientes)\n",
    "\n",
    "\n",
    "# ===================== SQL PERSONA NATURAL =====================\n",
    "\n",
    "sql_personas = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for id_cliente in personas:\n",
    "    sql_personas += (\n",
    "        \"INSERT INTO persona_natural (id_persona, cliente_id) \"\n",
    "        f\"VALUES ({id_cliente}, {id_cliente});\\n\"\n",
    "    )\n",
    "\n",
    "sql_personas += \"\\n-- migrate:down\\nDELETE FROM persona_natural;\\n\"\n",
    "\n",
    "with open(\"inserts_persona_natural.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_personas)\n",
    "\n",
    "\n",
    "# ===================== SQL EMPRESA =====================\n",
    "\n",
    "sql_empresas = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for id_cliente in empresas:\n",
    "    sql_empresas += (\n",
    "        \"INSERT INTO empresa (id_empresa, cliente_id) \"\n",
    "        f\"VALUES ({id_cliente}, {id_cliente});\\n\"\n",
    "    )\n",
    "\n",
    "sql_empresas += \"\\n-- migrate:down\\nDELETE FROM empresa;\\n\"\n",
    "\n",
    "with open(\"inserts_empresa.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_empresas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado vehiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# IMPORTAR DICCIONARIOS\n",
    "# ===============================\n",
    "\n",
    "from colors import dict_color\n",
    "from modelo import dict_modelo\n",
    "from gps import gps_dict, limpiar_gps, detectar_gps_base\n",
    "from clientes import dict_cliente, limpiar_cliente\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# FUNCIONES\n",
    "# ===============================\n",
    "\n",
    "def limpiar(texto):\n",
    "    if pd.isna(texto):\n",
    "        return None\n",
    "    return str(texto).strip().replace(\"'\", \"''\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# LEER CSV\n",
    "# ===============================\n",
    "\n",
    "df = pd.read_csv(\"ASP.csv\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# EXTRAER DATOS ÚNICOS\n",
    "# ===============================\n",
    "\n",
    "vehiculos = df[['PLACA', 'VIN', 'COLOR', 'MODELO', 'GPS', 'CLIENTE']].drop_duplicates().to_numpy()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# GENERAR INSERTS\n",
    "# ===============================\n",
    "\n",
    "text = '-- migrate:up \\n\\n'\n",
    "\n",
    "for v in vehiculos:\n",
    "\n",
    "    placa = limpiar(v[0])\n",
    "    vin = limpiar(v[1])\n",
    "\n",
    "    color_text = limpiar(v[2])\n",
    "    modelo_text = limpiar(v[3])\n",
    "    gps_text = limpiar_gps(v[4])\n",
    "    cliente_text = limpiar_cliente(v[5])\n",
    "\n",
    "    color_id = dict_color.get(color_text)\n",
    "    modelo_id = dict_modelo.get(modelo_text)\n",
    "    gps_base = detectar_gps_base(gps_text)\n",
    "    gps_id = gps_dict.get(gps_base)\n",
    "    cliente_id = dict_cliente.get(cliente_text)\n",
    "\n",
    "    # Validación de datos faltantes\n",
    "    if None in (color_id, modelo_id, gps_id, cliente_id):\n",
    "        print(f\"Error en datos: {placa}, {vin}\")\n",
    "        continue\n",
    "\n",
    "    text += f\"\"\"INSERT INTO vehiculos \n",
    "(placa, vin, color_id, modelo_id, gps_id, cliente_id)\n",
    "VALUES ('{placa}', '{vin}', {color_id}, {modelo_id}, {gps_id}, {cliente_id});\n",
    "\"\"\"\n",
    "\n",
    "text += '\\n-- migrate:down \\n\\nDELETE FROM vehiculos;'\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# GUARDAR ARCHIVO\n",
    "# ===============================\n",
    "\n",
    "with open('inserts_vehiculos.sql', 'w', encoding='utf-8') as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ===============================\n",
    "# Cargar dataset\n",
    "# ===============================\n",
    "\n",
    "ds = pd.read_csv(\"ASP.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ===============================\n",
    "# FUNCIONES\n",
    "# ===============================\n",
    "\n",
    "def limpiar_texto(t):\n",
    "    if not isinstance(t, str):\n",
    "        return None\n",
    "    t = t.strip().upper()\n",
    "    return t if t and t != \"NAN\" else None\n",
    "\n",
    "\n",
    "def extraer_numero_cliente(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return None\n",
    "    m = re.match(r\"\\((\\d+)\\)\", texto.strip())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# REQUISITOS PREVIOS\n",
    "# ===============================\n",
    "\n",
    "# Deben existir previamente:\n",
    "# colors         -> lista ordenada de colores\n",
    "# modelos_dict   -> dict { texto_modelo : id_modelo }\n",
    "# gps_dict       -> dict { texto_gps : id_gps }\n",
    "# clientes_dict  -> dict { dni/ruc : id_cliente }\n",
    "\n",
    "# ===============================\n",
    "# GENERACIÓN INSERT VEHÍCULOS\n",
    "# ===============================\n",
    "\n",
    "sql_vehiculos = \"-- migrate:up\\n\\n\"\n",
    "id_vehiculo = 1\n",
    "\n",
    "for _, row in ds.iterrows():\n",
    "\n",
    "    placa  = limpiar_texto(row['PLACA'])\n",
    "    vin    = limpiar_texto(row['VIN'])\n",
    "    color  = limpiar_texto(row['Color'])\n",
    "    modelo = limpiar_texto(row['Modelo'])\n",
    "    gps    = limpiar_texto(row['GPS'])\n",
    "\n",
    "    cliente_num = extraer_numero_cliente(row['Cliente'])\n",
    "\n",
    "    if not all([placa, vin, color, modelo, cliente_num]):\n",
    "        continue\n",
    "\n",
    "    # ===== COLOR → LISTA =====\n",
    "    try:\n",
    "        color_id = colors.index(color) + 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # ===== MODELO → DICCIONARIO =====\n",
    "    modelo_id = modelos.get(modelo)\n",
    "    if not modelo_id:\n",
    "        continue\n",
    "\n",
    "    # ===== GPS → DICCIONARIO =====\n",
    "    gps_id = gps_dict.get(gps) if gps else None\n",
    "\n",
    "    # ===== CLIENTE → DICCIONARIO =====\n",
    "    cliente_id = dict_cliente.get(cliente_num)\n",
    "    if not cliente_id:\n",
    "        continue\n",
    "\n",
    "    gps_sql = \"NULL\" if gps_id is None else gps_id\n",
    "\n",
    "    sql_vehiculos += (\n",
    "        \"INSERT INTO vehiculos \"\n",
    "        \"(id, placa, vin, colores_id, modelos_id, gps_id, clientes_id) \"\n",
    "        f\"VALUES ({id_vehiculo}, '{placa}', '{vin}', \"\n",
    "        f\"{color_id}, {modelo_id}, {gps_sql}, {cliente_id});\\n\"\n",
    "    )\n",
    "\n",
    "    id_vehiculo += 1\n",
    "\n",
    "\n",
    "sql_vehiculos += \"\\n-- migrate:down\\nDELETE FROM vehiculos;\\n\"\n",
    "\n",
    "with open(\"inserts_vehiculos.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_vehiculos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado de recepciones_campañas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# Cargar dataset\n",
    "# ===============================\n",
    "\n",
    "ds = pd.read_csv(\"ASP.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ===============================\n",
    "# CAMPAÑAS\n",
    "# ===============================\n",
    "\n",
    "campanias = {}\n",
    "id_campania = 1\n",
    "\n",
    "for c in ds['CAMPAÑA']:\n",
    "    if not isinstance(c, str):\n",
    "        continue\n",
    "\n",
    "    c = c.strip().upper()\n",
    "\n",
    "    if c and c not in campanias:\n",
    "        campanias[c] = id_campania\n",
    "        id_campania += 1\n",
    "\n",
    "# ===============================\n",
    "# RECEPCIONES\n",
    "# ===============================\n",
    "\n",
    "recepciones = {}\n",
    "id_recepcion = 1\n",
    "\n",
    "for f in ds['FECHA DE RECEPCION DEL VEHICULO']:\n",
    "    if pd.isna(f):\n",
    "        continue\n",
    "\n",
    "    f = str(f).strip()\n",
    "\n",
    "    if f not in recepciones:\n",
    "        recepciones[f] = id_recepcion\n",
    "        id_recepcion += 1\n",
    "\n",
    "# ===============================\n",
    "# RELACION N:M\n",
    "# ===============================\n",
    "\n",
    "relaciones = set()\n",
    "\n",
    "for _, row in ds.iterrows():\n",
    "\n",
    "    camp = str(row['CAMPAÑA']).strip().upper()\n",
    "    fecha = str(row['FECHA DE RECEPCION DEL VEHICULO']).strip()\n",
    "\n",
    "    if not camp or camp == \"NAN\" or not fecha or fecha == \"NAN\":\n",
    "        continue\n",
    "\n",
    "    relaciones.add((campanias[camp], recepciones[fecha]))\n",
    "\n",
    "# ===============================\n",
    "# GENERAR SQL\n",
    "# ===============================\n",
    "\n",
    "sql_rel = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for camp_id, rec_id in sorted(relaciones):\n",
    "    sql_rel += (\n",
    "        \"INSERT INTO recepciones_campañas (campañas_id, recepciones_id) \"\n",
    "        f\"VALUES ({camp_id}, {rec_id});\\n\"\n",
    "    )\n",
    "\n",
    "sql_rel += \"\\n-- migrate:down\\nDELETE FROM recepciones_campañas;\\n\"\n",
    "\n",
    "with open(\"inserts_recepciones_campañas.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_rel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado de ubicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# Cargar dataset\n",
    "# ===============================\n",
    "\n",
    "ds = pd.read_csv(\"ASP.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ===============================\n",
    "# LIMPIEZA\n",
    "# ===============================\n",
    "\n",
    "def limpiar_texto(txt):\n",
    "    if not isinstance(txt, str):\n",
    "        return None\n",
    "    return txt.strip().upper()\n",
    "\n",
    "# ===============================\n",
    "# DISTRITOS (CATALOGO)\n",
    "# ===============================\n",
    "\n",
    "distritos = {}\n",
    "id_distrito = 1\n",
    "\n",
    "for u in ds['UBICACIÓN']:\n",
    "    u = limpiar_texto(u)\n",
    "    if not u:\n",
    "        continue\n",
    "\n",
    "    if u not in distritos:\n",
    "        distritos[u] = id_distrito\n",
    "        id_distrito += 1\n",
    "\n",
    "# ===============================\n",
    "# UBICACIONES\n",
    "# ===============================\n",
    "\n",
    "ubicaciones = {}\n",
    "\n",
    "for u in ds['UBICACIÓN']:\n",
    "    u = limpiar_texto(u)\n",
    "    if not u:\n",
    "        continue\n",
    "\n",
    "    ubicaciones[u] = distritos[u]\n",
    "\n",
    "# ===============================\n",
    "# SQL DISTRITOS\n",
    "# ===============================\n",
    "\n",
    "sql_distritos = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for nombre, id_ in distritos.items():\n",
    "    sql_distritos += (\n",
    "        \"INSERT INTO distritos (id, nombre) \"\n",
    "        f\"VALUES ({id_}, '{nombre}');\\n\"\n",
    "    )\n",
    "\n",
    "sql_distritos += \"\\n-- migrate:down\\nDELETE FROM distritos;\\n\"\n",
    "\n",
    "with open(\"inserts_distritos.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_distritos)\n",
    "\n",
    "# ===============================\n",
    "# SQL UBICACIONES\n",
    "# ===============================\n",
    "\n",
    "sql_ubicaciones = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "for ubicacion, distrito_id in ubicaciones.items():\n",
    "    sql_ubicaciones += (\n",
    "        \"INSERT INTO ubicaciones (ubicacion, distritos_id) \"\n",
    "        f\"VALUES ('{ubicacion}', {distrito_id});\\n\"\n",
    "    )\n",
    "\n",
    "sql_ubicaciones += \"\\n-- migrate:down\\nDELETE FROM ubicaciones;\\n\"\n",
    "\n",
    "with open(\"inserts_ubicaciones.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_ubicaciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtrado telefono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "telefonos_rep = ds_ASP['Celular']\n",
    "\n",
    "telefonos = {}\n",
    "\n",
    "def telefono_valido(t):\n",
    "  if not isinstance(t, str):\n",
    "    return False\n",
    "  t = t.strip()\n",
    "  return re.fullmatch(r\"\\d{9}\", t) is not None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in telefonos_rep:\n",
    "  if not telefono_valido(t):\n",
    "    continue\n",
    "\n",
    "  numero = int(t)\n",
    "  telefonos[numero] = True  # evita duplicados\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_tel = \"-- migrate:up\\n\\n\"\n",
    "id_telefono = 1\n",
    "\n",
    "cliente_id = 1  # correlativo simple\n",
    "\n",
    "\n",
    "\n",
    "for numero in sorted(telefonos.keys()):\n",
    "\n",
    "  sql_tel += (\n",
    "\n",
    "    \"INSERT INTO telefonos (id_telefono, numero, cliente_id) \"\n",
    "\n",
    "    f\"VALUES ({id_telefono}, {numero}, {cliente_id});\\n\"\n",
    "\n",
    "  )\n",
    "\n",
    "  id_telefono += 1\n",
    "\n",
    "  cliente_id += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_tel += \"\\n-- migrate:down\\nDELETE FROM telefonos;\\n\"\n",
    "\n",
    "\n",
    "\n",
    "with open(\"inserts_telefono.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "  f.write(sql_tel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtrado formato pago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario generado (para usarlo después en pagos):\n",
      "{'AUTOPLAN': 1, 'AUTOPLAN VB ENTREGA+BCP': 2, 'BBVA': 3, 'BBVA BCP': 4, 'BBVA BCP INTBK': 5, 'BBVA BCP INTBK SCTBK': 6, 'BBVA BCP INTERBANK': 7, 'BBVA BCP ITBK': 8, 'BBVA BCP ITBK NIUBIZ': 9, 'BBVA BCP SCTBK': 10, 'BBVA IBK BCP': 11, 'BBVA INTBK': 12, 'BBVA INTBK BCP': 13, 'BBVA INTERBANK': 14, 'BBVA ITBK': 15, 'BBVA NIUBIZ': 16, 'BBVA SCOTBK': 17, 'BBVA SCOTIABANK': 18, 'BBVA SCOTIABANK MAF': 19, 'BBVA SCTBK BCP': 20, 'BBVA VISA': 21, 'BCP': 22, 'BCP BBVA': 23, 'BCP BBVA INTBK': 24, 'BCP BBVA MAF': 25, 'BCP INTBK': 26, 'BCP INTBK FONBIENES': 27, 'BCP INTBK NIUBIZ': 28, 'BCP INTERBANK': 29, 'BCP INTERBANK SCOTIABANK BBVA': 30, 'BCP ITBK': 31, 'BCP ITBK NIUBIZ': 32, 'BCP NIUBIZ': 33, 'BCP NIUBIZ+AUTOPLAN PEDIR VB': 34, 'BCP SANTANDER': 35, 'BCP SCOTBK': 36, 'BCP SCOTIABANK': 37, 'BCP SCTBK': 38, 'BCP SCTBK INTBK': 39, 'BCP VISA': 40, 'BCP VISA+PANDERO': 41, 'BCP+AUTOPLAN': 42, 'BCP+AUTOPLAN PEDIR VB PARA ENTREGA': 43, 'BCP+MAQUIMAS': 44, 'BCP+NIUBIZ': 45, 'BCP+PANDERO': 46, 'CREDITO BBVA': 47, 'CREDITO INTBK': 48, 'CREDITO VEHICULAR BBVA': 49, 'CREDITO VEHICULAR BCP': 50, 'CSCTBK BBVA': 51, 'IBK': 52, 'INTBK': 53, 'INTBK BBVA': 54, 'INTBK BBVA BCP': 55, 'INTBK BCP': 56, 'INTBK BCP BBVA': 57, 'INTBK SANTANDER': 58, 'INTERBANK': 59, 'INTERBANK BBVA': 60, 'INTERBANK BBVA BCP': 61, 'INTERBANK BBVA MAF': 62, 'INTERBANK BCP': 63, 'INTERBANK BCP BBVA': 64, 'INTERBANK NIUBIZ': 65, 'INTERBANK VISA': 66, 'ITBK': 67, 'LEASING BBVA': 68, 'LEASING BCP': 69, 'LEASING INTERBANK': 70, 'LEASING SCOTIABANK': 71, 'LEASING SCTBK BCP': 72, 'MAF': 73, 'MAF BBVA': 74, 'MAF BBVA BCP': 75, 'MAF BBVA NIUBIS': 76, 'MAF BCP BBVA': 77, 'MAF INTBK BBVA': 78, 'MAF+BBVA BCP': 79, 'MAF+BBVA BCP INTBK': 80, 'MAF+BBVA INTBK BCP': 81, 'MAF+BCP BBVA': 82, 'MAQUIMAS': 83, 'MAQUIMAS PEDIR PERMISO': 84, 'MAQUISITEMA BCP': 85, 'NIUBIZ': 86, 'NIUBIZ BBVA': 87, 'NIUBIZ BCP': 88, 'NIUBIZ IBK': 89, 'NIUBIZ SCOTIABANK': 90, 'NIUBIZ+RETOMA': 91, 'PANDERO': 92, 'PANDERO VISA FONBIENES': 93, 'PEDIR VB FONBIENES': 94, 'PEDIR VB PANDERO+BCP': 95, 'PEDIR VB TOTAL': 96, 'PROMOTORA OPCION+BCP': 97, 'RETOMA': 98, 'RETOMA BBVA': 99, 'RETOMA BBVA INTBK': 100, 'RETOMA BCP': 101, 'RETOMA BCP AUTOPLAN PEDI VB': 102, 'RETOMA BCP ITB NIUBIZ': 103, 'RETOMA BCP NIUBIZ': 104, 'RETOMA CREDITO VEHICULAR BBVA': 105, 'RETOMA FORD': 106, 'RETOMA INTBK': 107, 'RETOMA INTERBANK': 108, 'RETOMA MAF BBVA BCP': 109, 'RETOMA NIUBIZ': 110, 'RETOMA NIUBIZ BCP': 111, 'RETOMA SANTANDER': 112, 'RETOMA SANTANDER BBVA BCP': 113, 'RETOMA SANTANDER BCP': 114, 'RETOMA SANTANDER INTBK': 115, 'RETOMA SCOTIABANK': 116, 'RETOMA SCTBK': 117, 'RETOMA VISA': 118, 'RETOMA+BBVA': 119, 'RETOMA+BBVA BCP': 120, 'RETOMA+BBVA SCTBK': 121, 'RETOMA+BCP': 122, 'RETOMA+BCP BBVA': 123, 'RETOMA+BCP INTBK': 124, 'RETOMA+BCP ITBK': 125, 'RETOMA+BCP PEDIR VB MAQUIMAS': 126, 'RETOMA+BCP SCTBK': 127, 'RETOMA+BCP VISA': 128, 'RETOMA+BCP+SANTANDER': 129, 'RETOMA+CREDITO BBVA': 130, 'RETOMA+CREDITO VEHICULAR BBVA': 131, 'RETOMA+CREDITO VEHICULAR BCP': 132, 'RETOMA+INTBK': 133, 'RETOMA+INTERBANK': 134, 'RETOMA+INTERBANK BCP VISA': 135, 'RETOMA+LEASING BANBIF': 136, 'RETOMA+MAF BCP BBVA': 137, 'RETOMA+NIUBIZ': 138, 'RETOMA+NIUBIZ SCOTBK': 139, 'RETOMA+SANTANDER': 140, 'RETOMA+SANTANDER BBVA': 141, 'RETOMA+SANTANDER BCP': 142, 'RETOMA+SANTANDER BCP BBVA': 143, 'RETOMA+SANTANDER IBK': 144, 'RETOMA+SANTANDER ITBK': 145, 'RETOMA+SANTANDER SCOTIABANK': 146, 'RETOMA+SCOTIABANK': 147, 'RETOMA+SCTBK': 148, 'RETOMA+SCTBK INTBK': 149, 'RETOMA+VISA': 150, 'RETOMA+VISA BBVA': 151, 'SANTANDER': 152, 'SANTANDER BBVA': 153, 'SANTANDER BBVA BCP': 154, 'SANTANDER BBVA BCP+RETOMA': 155, 'SANTANDER BBVA SCTBK': 156, 'SANTANDER BCP': 157, 'SANTANDER BCP BBVA': 158, 'SANTANDER BCP BBVA SCTBK': 159, 'SANTANDER BCP ITB': 160, 'SANTANDER BCP ITBK': 161, 'SANTANDER BCP VISA': 162, 'SANTANDER INTBK': 163, 'SANTANDER INTBK BCP': 164, 'SANTANDER INTBK VISA': 165, 'SANTANDER INTERBANK': 166, 'SANTANDER ITBK': 167, 'SANTANDER NIUBIZ': 168, 'SANTANDER NIUBIZ BBVA': 169, 'SANTANDER NIUBIZ BCP': 170, 'SANTANDER RETOMA SCOTBK': 171, 'SANTANDER RETOMA+BCP': 172, 'SANTANDER SCTBK': 173, 'SANTANDER SCTBK BCP': 174, 'SANTANDER VISA': 175, 'SANTANDER+BBVA SCOT': 176, 'SANTANDER+BCP': 177, 'SANTANDER+BCP BBVA': 178, 'SANTANDER+BCP INTBK': 179, 'SANTANDER+BCP SCTBK': 180, 'SANTANDER+INTERBANK': 181, 'SANTANDER+NIUBIZ': 182, 'SANTANDER+RETOMA': 183, 'SANTANDER+RETOMA BCP': 184, 'SANTANDER+RETOMA SCOT': 185, 'SANTANDER+SCOT BBVA': 186, 'SANTANDER+SCTBK': 187, 'SCOTBK BBVA BCP': 188, 'SCOTBK BCP+PANDERO': 189, 'SCOTIABANK': 190, 'SCOTIABANK BBVA INTERBANK': 191, 'SCOTIABANK SANTANDER': 192, 'SCTBK': 193, 'VISA': 194, 'VISA NIUBIZ': 195}\n",
      "\n",
      "Total de formatos únicos: 195\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "formas_unicas = ds_ASP['FORMA DE PAGO'].dropna().unique()\n",
    "\n",
    "formas_unicas = sorted(formas_unicas)  # orden alfabético como el profe\n",
    "\n",
    "\n",
    "\n",
    "formatos_diccionario = {}\n",
    "\n",
    "\n",
    "\n",
    "text = '-- migrate:up \\n\\n'\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for forma in formas_unicas:\n",
    "\n",
    "    forma_escaped = str(forma).replace(\"'\", \"''\").strip()\n",
    "\n",
    "    if forma_escaped:  # evita vacíos\n",
    "\n",
    "        text += f\"INSERT INTO formatos_pago (id, nombre) VALUES ({id_counter}, '{forma_escaped}');\\n\"\n",
    "\n",
    "        formatos_diccionario[forma_escaped] = id_counter\n",
    "\n",
    "        id_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "text += '\\n-- migrate:down \\n\\nDELETE FROM formatos_pago;'\n",
    "\n",
    "\n",
    "\n",
    "with open('inserts_formatos_pago.sql', 'w', encoding='utf-8') as f:\n",
    "\n",
    "    f.write(text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Diccionario generado (para usarlo después en pagos):\")\n",
    "\n",
    "print(formatos_diccionario)\n",
    "\n",
    "print(f\"\\nTotal de formatos únicos: {len(formatos_diccionario)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtrado campañas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "campanas_rep = ds_ASP['CAMPAÑA']\n",
    "\n",
    "\n",
    "\n",
    "campanas = {}\n",
    "\n",
    "\n",
    "\n",
    "def campaña_valida(nombre):\n",
    "\n",
    "  if not isinstance(nombre, str):\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "  nombre = nombre.strip().upper()\n",
    "\n",
    "\n",
    "\n",
    "  if nombre == \"\" or nombre in {\"SIN CAMPAÑA\", \"NO APLICA\"}:\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "  return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for c in campanas_rep:\n",
    "\n",
    "  if not campaña_valida(c):\n",
    "\n",
    "    continue\n",
    "\n",
    "\n",
    "\n",
    "  nombre = c.strip().upper()\n",
    "\n",
    "  campanas[nombre] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_cam = \"-- migrate:up\\n\\n\"\n",
    "\n",
    "id_campaña = 1\n",
    "\n",
    "\n",
    "\n",
    "for nombre in sorted(campanas.keys()):\n",
    "\n",
    "  # descuento asignado por criterio\n",
    "\n",
    "  descuento = 0.10\n",
    "\n",
    "\n",
    "\n",
    "  sql_cam += (\n",
    "\n",
    "    \"INSERT INTO campañas (id_campaña, nombre, descuento) \"\n",
    "\n",
    "    f\"VALUES ({id_campaña}, '{nombre}', {descuento});\\n\"\n",
    "\n",
    "  )\n",
    "\n",
    "  id_campaña += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_cam += \"\\n-- migrate:down\\nDELETE FROM campañas;\\n\"\n",
    "\n",
    "\n",
    "\n",
    "with open(\"inserts_campañas.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "  f.write(sql_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bancos únicos encontrados: ['AUTOPLAN', 'BANBIF', 'BBVA', 'BCP', 'FONBIENES', 'IBK', 'INTBK', 'INTERBANK', 'ITBK', 'LEASING', 'MAF', 'MAQUIMAS', 'MAQUISITEMA', 'NIUBIS', 'NIUBIZ', 'PANDERO', 'PROMOTORA OPCION', 'SANTANDER', 'SCOTBK', 'SCOTIABANK', 'SCTBK', 'VISA']\n",
      "Diccionario de bancos: {'AUTOPLAN': 1, 'BANBIF': 2, 'BBVA': 3, 'BCP': 4, 'FONBIENES': 5, 'IBK': 6, 'INTBK': 7, 'INTERBANK': 8, 'ITBK': 9, 'LEASING': 10, 'MAF': 11, 'MAQUIMAS': 12, 'MAQUISITEMA': 13, 'NIUBIS': 14, 'NIUBIZ': 15, 'PANDERO': 16, 'PROMOTORA OPCION': 17, 'SANTANDER': 18, 'SCOTBK': 19, 'SCOTIABANK': 20, 'SCTBK': 21, 'VISA': 22}\n",
      "Total de bancos únicos: 22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Lista de bancos/entidades financieras conocidas (ajustada a tus datos)\n",
    "\n",
    "bancos_conocidos = [\n",
    "\n",
    "    'BCP', 'BBVA', 'SANTANDER', 'INTERBANK', 'INTBK', 'ITBK', 'IBK',\n",
    "\n",
    "    'SCOTIABANK', 'SCTBK', 'SCOTBK', 'NIUBIZ', 'NIUBIS',\n",
    "\n",
    "    'MAF', 'VISA', 'PANDERO', 'LEASING', 'AUTOPLAN', 'FONBIENES',\n",
    "\n",
    "    'MAQUIMAS', 'MAQUISITEMA', 'PROMOTORA OPCION', 'BANBIF'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def extraer_bancos(texto):\n",
    "\n",
    "    if pd.isna(texto):\n",
    "\n",
    "        return set()\n",
    "\n",
    "    texto_upper = str(texto).upper()\n",
    "\n",
    "    encontrados = set()\n",
    "\n",
    "    for banco in bancos_conocidos:\n",
    "\n",
    "        # Busca palabra completa o casi completa\n",
    "\n",
    "        if re.search(r'\\b' + re.escape(banco) + r'\\b', texto_upper) or banco in texto_upper:\n",
    "\n",
    "            encontrados.add(banco)\n",
    "\n",
    "    return encontrados\n",
    "\n",
    "\n",
    "\n",
    "# Colectar todos los bancos únicos\n",
    "\n",
    "all_bancos = set()\n",
    "\n",
    "for forma in ds_ASP['FORMA DE PAGO'].dropna():\n",
    "\n",
    "    all_bancos.update(extraer_bancos(forma))\n",
    "\n",
    "\n",
    "\n",
    "bancos_unicos = sorted(list(all_bancos))\n",
    "\n",
    "\n",
    "\n",
    "# Generar SQL\n",
    "\n",
    "bancos_diccionario = {}\n",
    "\n",
    "text = '-- migrate:up \\n\\n'\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for banco in bancos_unicos:\n",
    "\n",
    "    banco_escaped = banco.replace(\"'\", \"''\")\n",
    "\n",
    "    text += f\"INSERT INTO bancos (id, nombre) VALUES ({id_counter}, '{banco_escaped}');\\n\"\n",
    "\n",
    "    bancos_diccionario[banco] = id_counter\n",
    "\n",
    "    id_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "text += '\\n-- migrate:down \\n\\nDELETE FROM bancos;'\n",
    "\n",
    "\n",
    "\n",
    "with open('inserts_bancos.sql', 'w', encoding='utf-8') as f:\n",
    "\n",
    "    f.write(text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Bancos únicos encontrados:\", bancos_unicos)\n",
    "\n",
    "print(\"Diccionario de bancos:\", bancos_diccionario)\n",
    "\n",
    "print(f\"Total de bancos únicos: {len(bancos_unicos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTRADO DISTRITOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# IMPORTS\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DICCIONARIO DE DISTRITOS (LIMA)\n",
    "# ===============================\n",
    "DIC_DISTRITOS = {\n",
    "    \"ATE\": \"Ate\",\n",
    "    \"SAN ISIDRO\": \"San Isidro\",\n",
    "    \"MIRAFLORES\": \"Miraflores\",\n",
    "    \"SURCO\": \"Santiago de Surco\",\n",
    "    \"SANTIAGO DE SURCO\": \"Santiago de Surco\",\n",
    "    \"LA MOLINA\": \"La Molina\",\n",
    "    \"MOLINA\": \"La Molina\",\n",
    "    \"CAMACHO\": \"La Molina\",\n",
    "    \"SAN BORJA\": \"San Borja\",\n",
    "    \"SJL\": \"San Juan de Lurigancho\",\n",
    "    \"SAN JUAN DE LURIGANCHO\": \"San Juan de Lurigancho\",\n",
    "    \"SJM\": \"San Juan de Miraflores\",\n",
    "    \"SAN JUAN DE MIRAFLORES\": \"San Juan de Miraflores\",\n",
    "    \"VES\": \"Villa El Salvador\",\n",
    "    \"VILLA EL SALVADOR\": \"Villa El Salvador\",\n",
    "    \"VMT\": \"Villa María del Triunfo\",\n",
    "    \"VILLA MARIA DEL TRIUNFO\": \"Villa María del Triunfo\",\n",
    "    \"LOS OLIVOS\": \"Los Olivos\",\n",
    "    \"COMAS\": \"Comas\",\n",
    "    \"INDEPENDENCIA\": \"Independencia\",\n",
    "    \"PUENTE PIEDRA\": \"Puente Piedra\",\n",
    "    \"CARABAYLLO\": \"Carabayllo\",\n",
    "    \"RIMAC\": \"Rímac\",\n",
    "    \"BREÑA\": \"Breña\",\n",
    "    \"PUEBLO LIBRE\": \"Pueblo Libre\",\n",
    "    \"MAGDALENA\": \"Magdalena del Mar\",\n",
    "    \"MAGDALENA DEL MAR\": \"Magdalena del Mar\",\n",
    "    \"LINCE\": \"Lince\",\n",
    "    \"JESUS MARIA\": \"Jesús María\",\n",
    "    \"BARRANCO\": \"Barranco\",\n",
    "    \"CHORRILLOS\": \"Chorrillos\",\n",
    "    \"SAN MIGUEL\": \"San Miguel\",\n",
    "    \"CALLAO\": \"Callao\"\n",
    "}\n",
    "\n",
    "DISTRITOS_ALEATORIOS = list(set(DIC_DISTRITOS.values()))\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# FUNCIÓN PARA OBTENER DISTRITO\n",
    "# ===============================\n",
    "def obtener_distrito(ubicacion):\n",
    "    if not isinstance(ubicacion, str):\n",
    "        return random.choice(DISTRITOS_ALEATORIOS)\n",
    "\n",
    "    texto = ubicacion.upper()\n",
    "\n",
    "    for clave, distrito in DIC_DISTRITOS.items():\n",
    "        if clave in texto:\n",
    "            return distrito\n",
    "\n",
    "    return random.choice(DISTRITOS_ALEATORIOS)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# LEER CSV\n",
    "# ===============================\n",
    "ds_ASP = pd.read_csv(\"ASP.csv\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# CREAR COLUMNA DISTRITO\n",
    "# ===============================\n",
    "ds_ASP[\"Distrito\"] = ds_ASP[\"UBICACIÓN\"].apply(obtener_distrito)\n",
    "ds_ASP = ds_ASP[ds_ASP[\"Distrito\"].notna()]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# FILTRADO PARA INSERT\n",
    "# ===============================\n",
    "distritos = {}\n",
    "\n",
    "for d in ds_ASP[\"Distrito\"]:\n",
    "    distritos[d] = True\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# GENERAR SQL INSERT\n",
    "# ===============================\n",
    "sql_dist = \"-- migrate:up\\n\\n\"\n",
    "id_distrito = 1\n",
    "\n",
    "for nombre in sorted(distritos.keys()):\n",
    "    sql_dist += (\n",
    "        \"INSERT INTO distritos (id_distrito, nombre) \"\n",
    "        f\"VALUES ({id_distrito}, '{nombre}');\\n\"\n",
    "    )\n",
    "    id_distrito += 1\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# GUARDAR ARCHIVO SQL\n",
    "# ===============================\n",
    "with open(\"inserts_distritos.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sql_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
